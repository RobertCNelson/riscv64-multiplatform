From f3ae4d09d9f55f081ca8a1c6bcb6f9fd6e3b31f8 Mon Sep 17 00:00:00 2001
From: Tom <support@vamrs.com>
Date: Fri, 8 Jan 2021 04:01:19 +0800
Subject: [PATCH 14/38] drivers/net/ethernet/stmicro: Add dcache flush
 functions for VIC7100

Note: including uSDK v0.9->v1.0 patch
---
 drivers/net/ethernet/stmicro/stmmac/descs.h   |   3 +
 .../net/ethernet/stmicro/stmmac/stmmac_main.c | 216 +++++++++++++++++-
 2 files changed, 216 insertions(+), 3 deletions(-)

diff --git a/drivers/net/ethernet/stmicro/stmmac/descs.h b/drivers/net/ethernet/stmicro/stmmac/descs.h
index 49d6a866244f..1bf6506ff778 100644
--- a/drivers/net/ethernet/stmicro/stmmac/descs.h
+++ b/drivers/net/ethernet/stmicro/stmmac/descs.h
@@ -169,6 +169,9 @@ struct dma_extended_desc {
 	__le32 des5;	/* Reserved */
 	__le32 des6;	/* Tx/Rx Timestamp Low */
 	__le32 des7;	/* Tx/Rx Timestamp High */
+#if defined(CONFIG_FPGA_GMAC_FLUSH_DDR)
+	__le32 pad[8];
+#endif
 };
 
 /* Enhanced descriptor for TBS */
diff --git a/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c b/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
index 4749bd0af160..d1f961e2ba4e 100644
--- a/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
+++ b/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
@@ -113,6 +113,20 @@ static void stmmac_exit_fs(struct net_device *dev);
 
 #define STMMAC_COAL_TIMER(x) (ns_to_ktime((x) * NSEC_PER_USEC))
 
+#ifdef CONFIG_FPGA_GMAC_FLUSH_DDR
+#define FLUSH_RX_DESC_ENABLE
+#define FLUSH_RX_BUF_ENABLE
+
+#define FLUSH_TX_DESC_ENABLE
+#define FLUSH_TX_BUF_ENABLE
+
+#include <soc/starfive/vic7100.h>
+static inline void stmmac_flush_dcache(unsigned long start, unsigned long len)
+{
+	starfive_flush_dcache(_ALIGN_DOWN(start, 64), len + start % 64);
+}
+#endif
+
 /**
  * stmmac_verify_args - verify the driver parameters.
  * Description: it checks the driver parameters and set a default in case of
@@ -1237,6 +1251,16 @@ static void stmmac_clear_rx_descriptors(struct stmmac_priv *priv, u32 queue)
 					priv->use_riwt, priv->mode,
 					(i == priv->dma_rx_size - 1),
 					priv->dma_buf_sz);
+
+#ifdef FLUSH_RX_DESC_ENABLE
+	unsigned long len;
+	if (priv->extend_desc)
+		len = DMA_DEFAULT_RX_SIZE * sizeof(struct dma_extended_desc);
+	else
+		len = DMA_DEFAULT_RX_SIZE * sizeof(struct dma_desc);
+
+	stmmac_flush_dcache(rx_q->dma_rx_phy, len);
+#endif
 }
 
 /**
@@ -1265,6 +1289,16 @@ static void stmmac_clear_tx_descriptors(struct stmmac_priv *priv, u32 queue)
 
 		stmmac_init_tx_desc(priv, p, priv->mode, last);
 	}
+
+#ifdef FLUSH_TX_DESC_ENABLE
+	unsigned long len;
+	if (priv->extend_desc)
+		len = DMA_DEFAULT_TX_SIZE * sizeof(struct dma_extended_desc);
+	else
+		len = DMA_DEFAULT_TX_SIZE * sizeof(struct dma_desc);
+
+	stmmac_flush_dcache(tx_q->dma_tx_phy, len);
+#endif
 }
 
 /**
@@ -1322,6 +1356,9 @@ static int stmmac_init_rx_buffers(struct stmmac_priv *priv, struct dma_desc *p,
 
 	buf->addr = page_pool_get_dma_addr(buf->page);
 	stmmac_set_desc_addr(priv, p, buf->addr);
+#ifdef FLUSH_RX_BUF_ENABLE
+	stmmac_flush_dcache(buf->addr, priv->dma_buf_sz);
+#endif
 	if (priv->dma_buf_sz == BUF_SIZE_16KiB)
 		stmmac_init_desc3(priv, p);
 
@@ -1505,6 +1542,15 @@ static int init_dma_tx_desc_rings(struct net_device *dev)
 			tx_q->tx_skbuff_dma[i].last_segment = false;
 			tx_q->tx_skbuff[i] = NULL;
 		}
+#ifdef FLUSH_TX_DESC_ENABLE
+		unsigned long len;
+		if (priv->extend_desc)
+			len = DMA_DEFAULT_TX_SIZE * sizeof(struct dma_extended_desc);
+		else
+			len = DMA_DEFAULT_TX_SIZE * sizeof(struct dma_desc);
+
+		stmmac_flush_dcache(tx_q->dma_tx_phy, len);
+#endif
 
 		tx_q->dirty_tx = 0;
 		tx_q->cur_tx = 0;
@@ -2026,7 +2072,21 @@ static int stmmac_tx_clean(struct stmmac_priv *priv, int budget, u32 queue)
 				&priv->xstats, p, priv->ioaddr);
 		/* Check if the descriptor is owned by the DMA */
 		if (unlikely(status & tx_dma_own))
+		{
+#ifdef FLUSH_TX_DESC_ENABLE
+		unsigned long start, len;
+		if (priv->extend_desc) {
+			start = tx_q->dma_tx_phy + entry * sizeof(struct dma_extended_desc);
+			len = sizeof(struct dma_extended_desc);
+		} else {
+			start = tx_q->dma_tx_phy + entry * sizeof(struct dma_desc);
+			len = sizeof(struct dma_desc);
+		}
+
+		stmmac_flush_dcache(start, len);
+#endif
 			break;
+		}
 
 		count++;
 
@@ -2076,6 +2136,19 @@ static int stmmac_tx_clean(struct stmmac_priv *priv, int budget, u32 queue)
 		}
 
 		stmmac_release_tx_desc(priv, p, priv->mode);
+#ifdef FLUSH_TX_DESC_ENABLE
+		/*wangyh for test,flush description*/
+		unsigned long start, len;
+		if (priv->extend_desc) {
+			start = tx_q->dma_tx_phy + entry * sizeof(struct dma_extended_desc);
+			len = sizeof(struct dma_extended_desc);
+		} else {
+			start = tx_q->dma_tx_phy + entry * sizeof(struct dma_desc);
+			len = sizeof(struct dma_desc);
+		}
+
+		stmmac_flush_dcache(start, len);
+#endif
 
 		entry = STMMAC_GET_ENTRY(entry, priv->dma_tx_size);
 	}
@@ -2125,6 +2198,16 @@ static void stmmac_tx_err(struct stmmac_priv *priv, u32 chan)
 	stmmac_stop_tx_dma(priv, chan);
 	dma_free_tx_skbufs(priv, chan);
 	stmmac_clear_tx_descriptors(priv, chan);
+
+#ifdef FLUSH_TX_DESC_ENABLE
+	unsigned long len;
+	if (priv->extend_desc)
+		len = DMA_DEFAULT_TX_SIZE * sizeof(struct dma_extended_desc);
+	else
+		len = DMA_DEFAULT_TX_SIZE * sizeof(struct dma_desc);
+
+	stmmac_flush_dcache(tx_q->dma_tx_phy, len);
+#endif
 	tx_q->dirty_tx = 0;
 	tx_q->cur_tx = 0;
 	tx_q->mss = 0;
@@ -3067,6 +3150,18 @@ static void stmmac_tso_allocator(struct stmmac_priv *priv, dma_addr_t des,
 				(last_segment) && (tmp_len <= TSO_MAX_BUFF_SIZE),
 				0, 0);
 
+#ifdef FLUSH_TX_DESC_ENABLE
+		unsigned long start, len;
+		if (priv->extend_desc) {
+			start = tx_q->dma_tx_phy + tx_q->cur_tx * sizeof(struct dma_extended_desc);
+			len = sizeof(struct dma_extended_desc);
+		} else {
+			start = tx_q->dma_tx_phy + tx_q->cur_tx * sizeof(struct dma_desc);
+			len = sizeof(struct dma_desc);
+		}
+
+		stmmac_flush_dcache(start, len);
+#endif
 		tmp_len -= TSO_MAX_BUFF_SIZE;
 	}
 }
@@ -3112,6 +3207,9 @@ static netdev_tx_t stmmac_tso_xmit(struct sk_buff *skb, struct net_device *dev)
 	u32 pay_len, mss;
 	dma_addr_t des;
 	int i;
+#ifdef FLUSH_TX_DESC_ENABLE
+	unsigned int mss_entry;
+#endif
 
 	tx_q = &priv->tx_queue[queue];
 	first_tx = tx_q->cur_tx;
@@ -3151,6 +3249,9 @@ static netdev_tx_t stmmac_tso_xmit(struct sk_buff *skb, struct net_device *dev)
 			mss_desc = &tx_q->dma_tx[tx_q->cur_tx];
 
 		stmmac_set_mss(priv, mss_desc, mss);
+#ifdef FLUSH_TX_DESC_ENABLE
+		mss_entry = tx_q->cur_tx;
+#endif
 		tx_q->mss = mss;
 		tx_q->cur_tx = STMMAC_GET_ENTRY(tx_q->cur_tx,
 						priv->dma_tx_size);
@@ -3185,6 +3286,10 @@ static netdev_tx_t stmmac_tso_xmit(struct sk_buff *skb, struct net_device *dev)
 	if (dma_mapping_error(priv->device, des))
 		goto dma_map_err;
 
+#ifdef FLUSH_TX_BUF_ENABLE
+	stmmac_flush_dcache(des, skb_headlen(skb));
+#endif
+
 	tx_q->tx_skbuff_dma[first_entry].buf = des;
 	tx_q->tx_skbuff_dma[first_entry].len = skb_headlen(skb);
 
@@ -3216,6 +3321,9 @@ static netdev_tx_t stmmac_tso_xmit(struct sk_buff *skb, struct net_device *dev)
 		if (dma_mapping_error(priv->device, des))
 			goto dma_map_err;
 
+#ifdef FLUSH_TX_BUF_ENABLE
+		stmmac_flush_dcache(des, skb_frag_size(frag));
+#endif
 		stmmac_tso_allocator(priv, des, skb_frag_size(frag),
 				     (i == nfrags - 1), queue);
 
@@ -3260,7 +3368,6 @@ static netdev_tx_t stmmac_tso_xmit(struct sk_buff *skb, struct net_device *dev)
 	 * ndo_start_xmit will fill this descriptor the next time it's
 	 * called and stmmac_tx_clean may clean up to this descriptor.
 	 */
-	tx_q->cur_tx = STMMAC_GET_ENTRY(tx_q->cur_tx, priv->dma_tx_size);
 
 	if (unlikely(stmmac_tx_avail(priv, queue) <= (MAX_SKB_FRAGS + 1))) {
 		netif_dbg(priv, hw, priv->dev, "%s: stop transmitted packets\n",
@@ -3291,6 +3398,18 @@ static netdev_tx_t stmmac_tso_xmit(struct sk_buff *skb, struct net_device *dev)
 			1, tx_q->tx_skbuff_dma[first_entry].last_segment,
 			hdr / 4, (skb->len - proto_hdr_len));
 
+#ifdef FLUSH_TX_DESC_ENABLE
+	unsigned long start, len;
+	if (priv->extend_desc) {
+		start = tx_q->dma_tx_phy + first_entry * sizeof(struct dma_extended_desc);
+		len = sizeof(struct dma_extended_desc);
+	} else {
+		start = tx_q->dma_tx_phy + first_entry * sizeof(struct dma_desc);
+		len = sizeof(struct dma_desc);
+	}
+
+	stmmac_flush_dcache(start, len);
+#endif
 	/* If context desc is used to change MSS */
 	if (mss_desc) {
 		/* Make sure that first descriptor has been completely
@@ -3300,6 +3419,18 @@ static netdev_tx_t stmmac_tso_xmit(struct sk_buff *skb, struct net_device *dev)
 		 */
 		dma_wmb();
 		stmmac_set_tx_owner(priv, mss_desc);
+#ifdef FLUSH_TX_DESC_ENABLE
+		unsigned long start, len;
+		if (priv->extend_desc) {
+			start = tx_q->dma_tx_phy + mss_entry * sizeof(struct dma_extended_desc);
+			len = sizeof(struct dma_extended_desc);
+		} else {
+			start = tx_q->dma_tx_phy + mss_entry * sizeof(struct dma_desc);
+			len = sizeof(struct dma_desc);
+		}
+
+		stmmac_flush_dcache(start, len);
+#endif
 	}
 
 	/* The own bit must be the latest setting done when prepare the
@@ -3438,6 +3569,9 @@ static netdev_tx_t stmmac_xmit(struct sk_buff *skb, struct net_device *dev)
 		if (dma_mapping_error(priv->device, des))
 			goto dma_map_err; /* should reuse desc w/o issues */
 
+#ifdef FLUSH_TX_BUF_ENABLE
+		 stmmac_flush_dcache(des, len);
+#endif
 		tx_q->tx_skbuff_dma[entry].buf = des;
 
 		stmmac_set_desc_addr(priv, desc, des);
@@ -3449,6 +3583,18 @@ static netdev_tx_t stmmac_xmit(struct sk_buff *skb, struct net_device *dev)
 		/* Prepare the descriptor and set the own bit too */
 		stmmac_prepare_tx_desc(priv, desc, 0, len, csum_insertion,
 				priv->mode, 1, last_segment, skb->len);
+#ifdef FLUSH_TX_DESC_ENABLE
+		unsigned long start, desc_len;
+		if (priv->extend_desc) {
+			start = tx_q->dma_tx_phy + entry * sizeof(struct dma_extended_desc);
+			desc_len = sizeof(struct dma_extended_desc);
+		} else {
+			start = tx_q->dma_tx_phy + entry * sizeof(struct dma_desc);
+			desc_len = sizeof(struct dma_desc);
+		}
+
+		stmmac_flush_dcache(start, desc_len);
+#endif
 	}
 
 	/* Only the last descriptor gets to point to the skb. */
@@ -3483,6 +3629,18 @@ static netdev_tx_t stmmac_xmit(struct sk_buff *skb, struct net_device *dev)
 
 		tx_q->tx_count_frames = 0;
 		stmmac_set_tx_ic(priv, desc);
+#ifdef FLUSH_TX_DESC_ENABLE
+		unsigned long start, len;
+		if (priv->extend_desc) {
+			start = tx_q->dma_tx_phy + entry * sizeof(struct dma_extended_desc);
+			len = sizeof(struct dma_extended_desc);
+		} else {
+			start = tx_q->dma_tx_phy + entry * sizeof(struct dma_desc);
+			len = sizeof(struct dma_desc);
+		}
+
+		stmmac_flush_dcache(start, len);
+#endif
 		priv->xstats.tx_set_ic_bit++;
 	}
 
@@ -3566,6 +3724,23 @@ static netdev_tx_t stmmac_xmit(struct sk_buff *skb, struct net_device *dev)
 
 	netdev_tx_sent_queue(netdev_get_tx_queue(dev, queue), skb->len);
 
+#ifdef FLUSH_TX_BUF_ENABLE
+	stmmac_flush_dcache(des, nopaged_len);
+#endif
+
+#ifdef FLUSH_TX_DESC_ENABLE
+	unsigned long start, len;
+	if (priv->extend_desc) {
+		start = tx_q->dma_tx_phy + first_entry * sizeof(struct dma_extended_desc);
+		len = sizeof(struct dma_extended_desc);
+	} else {
+		start = tx_q->dma_tx_phy + first_entry * sizeof(struct dma_desc);
+		len = sizeof(struct dma_desc);
+	}
+
+	stmmac_flush_dcache(start, len);
+#endif
+
 	stmmac_enable_dma_transmission(priv, priv->ioaddr);
 
 	if (likely(priv->extend_desc))
@@ -3676,9 +3851,22 @@ static inline void stmmac_rx_refill(struct stmmac_priv *priv, u32 queue)
 		if (!priv->use_riwt)
 			use_rx_wd = false;
 
-		dma_wmb();
 		stmmac_set_rx_owner(priv, p, use_rx_wd);
 
+#ifdef FLUSH_RX_DESC_ENABLE
+		unsigned long start, len;
+		if (priv->extend_desc) {
+			start = rx_q->dma_rx_phy + entry * sizeof(struct dma_extended_desc);
+			len = sizeof(struct dma_extended_desc);
+		} else {
+			start = rx_q->dma_rx_phy + entry * sizeof(struct dma_desc);
+			len = sizeof(struct dma_desc);
+		}
+
+		stmmac_flush_dcache(start, len);
+#endif
+		dma_wmb();
+
 		entry = STMMAC_GET_ENTRY(entry, priv->dma_rx_size);
 	}
 	rx_q->dirty_rx = entry;
@@ -3806,8 +3994,21 @@ static int stmmac_rx(struct stmmac_priv *priv, int limit, u32 queue)
 		status = stmmac_rx_status(priv, &priv->dev->stats,
 				&priv->xstats, p);
 		/* check if managed by the DMA otherwise go ahead */
-		if (unlikely(status & dma_own))
+		if (unlikely(status & dma_own)) {
+#ifdef FLUSH_RX_DESC_ENABLE
+		unsigned long start, len;
+		if (priv->extend_desc) {
+			start = rx_q->dma_rx_phy + entry * sizeof(struct dma_extended_desc);
+			len = sizeof(struct dma_extended_desc);
+		} else {
+			start = rx_q->dma_rx_phy + entry * sizeof(struct dma_desc);
+			len = sizeof(struct dma_desc);
+		}
+
+		stmmac_flush_dcache(start, len);
+#endif
 			break;
+		}
 
 		rx_q->cur_rx = STMMAC_GET_ENTRY(rx_q->cur_rx,
 						priv->dma_rx_size);
@@ -3879,6 +4080,9 @@ static int stmmac_rx(struct stmmac_priv *priv, int limit, u32 queue)
 
 			dma_sync_single_for_cpu(priv->device, buf->addr,
 						buf1_len, DMA_FROM_DEVICE);
+#ifdef FLUSH_RX_BUF_ENABLE
+				stmmac_flush_dcache(buf->addr, buf1_len);
+#endif
 			skb_copy_to_linear_data(skb, page_address(buf->page),
 						buf1_len);
 			skb_put(skb, buf1_len);
@@ -3889,6 +4093,9 @@ static int stmmac_rx(struct stmmac_priv *priv, int limit, u32 queue)
 		} else if (buf1_len) {
 			dma_sync_single_for_cpu(priv->device, buf->addr,
 						buf1_len, DMA_FROM_DEVICE);
+#ifdef FLUSH_RX_BUF_ENABLE
+				stmmac_flush_dcache(buf->addr, buf1_len);
+#endif
 			skb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags,
 					buf->page, 0, buf1_len,
 					priv->dma_buf_sz);
@@ -3901,6 +4108,9 @@ static int stmmac_rx(struct stmmac_priv *priv, int limit, u32 queue)
 		if (buf2_len) {
 			dma_sync_single_for_cpu(priv->device, buf->sec_addr,
 						buf2_len, DMA_FROM_DEVICE);
+#ifdef FLUSH_RX_BUF_ENABLE
+				stmmac_flush_dcache(buf->sec_addr, buf2_len);
+#endif
 			skb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags,
 					buf->sec_page, 0, buf2_len,
 					priv->dma_buf_sz);
-- 
2.30.0

